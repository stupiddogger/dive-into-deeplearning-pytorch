{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.word2vec\n",
    "\n",
    "RNN使用one-hot表示向量，构造容易，但无法准确表达不同词之间的相似度，通常情况下使用余弦相似度。\n",
    "\n",
    "word2vec是一个常用的词嵌入工具，它将每个词表示成定长的向量，并通过语料库上的预训练使得这些向量可以较好地表达不同词之间的相似和类比关系，以引入一定的语音信息。\n",
    "\n",
    "### 1.1 Skip-Gram\n",
    "\n",
    "假设背景词由中心词生成，建模P(w_o|w_c)，其中w_c为中心词，w_o为任一背景词\n",
    "\n",
    "### 1.2 CBOW\n",
    "\n",
    "假设背景词由中心词生成，建模P(w_c|W_o)，其中w_c为中心词，W_o为中心词集合\n",
    "\n",
    "### Skip-Gram实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as Data\n",
    "\n",
    "##载入数据集，使用PTB数据集，未下载到\n",
    "with open('/home/kesci/input/ptb_train1020/ptb.train.txt', 'r') as f:\n",
    "    lines = f.readlines() # 该数据集中句子以换行符为分割\n",
    "    raw_dataset = [st.split() for st in lines] # st是sentence的缩写，单词以空格为分割\n",
    "print('# sentences: %d' % len(raw_dataset))\n",
    "\n",
    "#建立词语索引\n",
    "counter = collections.Counter([tk for st in raw_dataset for tk in st]) # tk是token的缩写\n",
    "counter = dict(filter(lambda x: x[1] >= 5, counter.items())) # 只保留在数据集中至少出现5次的词\n",
    "\n",
    "idx_to_token = [tk for tk, _ in counter.items()]\n",
    "token_to_idx = {tk: idx for idx, tk in enumerate(idx_to_token)}\n",
    "dataset = [[token_to_idx[tk] for tk in st if tk in token_to_idx]\n",
    "           for st in raw_dataset] # raw_dataset中的单词在这一步被转换为对应的idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 二次采样\n",
    "\n",
    "文本数据中会存在一些a\\the\\in等高频词。一般情况下，一个词和低频词同时出现比和较高频次同时出现训练队词嵌入模型更为有利。因此词嵌入模型可以对词进行二次采样。具体来说，数据集中每个被索引词w_i将由一定概率被丢弃。\n",
    "\n",
    "丢弃概率为P(w_i)=max(1-sqrt(t/f(w_i),0),t为超参数，f(w_i)为数据集中w_i的个数与总词数之比。根据此概率，越高频的词越有可能被丢弃。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discard(idx):\n",
    "    '''\n",
    "    @params:\n",
    "        idx: 单词的下标\n",
    "    @return: True/False 表示是否丢弃该单词\n",
    "    '''\n",
    "    return random.uniform(0, 1) < 1 - math.sqrt(\n",
    "        1e-4 / counter[idx_to_token[idx]] * num_tokens)\n",
    "\n",
    "subsampled_dataset = [[tk for tk in st if not discard(tk)] for st in dataset]\n",
    "print('# tokens: %d' % sum([len(st) for st in subsampled_dataset]))\n",
    "\n",
    "def compare_counts(token):\n",
    "    return '# %s: before=%d, after=%d' % (token, sum(\n",
    "        [st.count(token_to_idx[token]) for st in dataset]), sum(\n",
    "        [st.count(token_to_idx[token]) for st in subsampled_dataset]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#提取背景词和中心词\n",
    "def get_centers_and_contexts(dataset, max_window_size):\n",
    "    '''\n",
    "    @params:\n",
    "        dataset: 数据集为句子的集合，每个句子则为单词的集合，此时单词已经被转换为相应数字下标\n",
    "        max_window_size: 背景词的词窗大小的最大值\n",
    "    @return:\n",
    "        centers: 中心词的集合\n",
    "        contexts: 背景词窗的集合，与中心词对应，每个背景词窗则为背景词的集合\n",
    "    '''\n",
    "    centers, contexts = [], []\n",
    "    for st in dataset:\n",
    "        if len(st) < 2:  # 每个句子至少要有2个词才可能组成一对“中心词-背景词”\n",
    "            continue\n",
    "        centers += st\n",
    "        for center_i in range(len(st)):\n",
    "            window_size = random.randint(1, max_window_size) # 随机选取背景词窗大小\n",
    "            indices = list(range(max(0, center_i - window_size),\n",
    "                                 min(len(st), center_i + 1 + window_size)))\n",
    "            indices.remove(center_i)  # 将中心词排除在背景词之外\n",
    "            contexts.append([st[idx] for idx in indices])\n",
    "    return centers, contexts\n",
    "\n",
    "all_centers, all_contexts = get_centers_and_contexts(subsampled_dataset, 5)\n",
    "\n",
    "tiny_dataset = [list(range(7)), list(range(7, 10))]\n",
    "print('dataset', tiny_dataset)\n",
    "for center, context in zip(*get_centers_and_contexts(tiny_dataset, 2)):\n",
    "    print('center', center, 'has contexts', context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#跳字模型实现\n",
    "#每个词被表示为两个d维向量，作为中心词时为v，作为背景词是为u\n",
    "\n",
    "#pytorch中定义embedding层\n",
    "embed = nn.Embedding(num_embeddings=10, embedding_dim=4)\n",
    "\n",
    "#skip-gram前向计算\n",
    "def skip_gram(center, contexts_and_negatives, embed_v, embed_u):\n",
    "    '''\n",
    "    @params:\n",
    "        center: 中心词下标，形状为 (n, 1) 的整数张量\n",
    "        contexts_and_negatives: 背景词和噪音词下标，形状为 (n, m) 的整数张量\n",
    "        embed_v: 中心词的 embedding 层\n",
    "        embed_u: 背景词的 embedding 层\n",
    "    @return:\n",
    "        pred: 中心词与背景词（或噪音词）的内积，之后可用于计算概率 p(w_o|w_c)\n",
    "    '''\n",
    "    v = embed_v(center) # shape of (n, 1, d)\n",
    "    u = embed_u(contexts_and_negatives) # shape of (n, m, d)\n",
    "    pred = torch.bmm(v, u.permute(0, 2, 1)) # bmm((n, 1, d), (n, d, m)) => shape of (n, 1, m)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax考虑到背景词可能是词典中的任意一个词，若词典中含的词过多，会导致计算量比较大。\n",
    "\n",
    "负采样可以解决该问题，对每一个中心词和背景词，从词典中随机采样K个噪声词。\n",
    "\n",
    "根据 Word2Vec 论文的建议，噪声词采样概率  P(w)  设为  w  词频与总词频之比的  0.75  次方。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negatives(all_contexts, sampling_weights, K):\n",
    "    '''\n",
    "    @params:\n",
    "        all_contexts: [[w_o1, w_o2, ...], [...], ... ]\n",
    "        sampling_weights: 每个单词的噪声词采样概率\n",
    "        K: 随机采样个数\n",
    "    @return:\n",
    "        all_negatives: [[w_n1, w_n2, ...], [...], ...]\n",
    "    '''\n",
    "    all_negatives, neg_candidates, i = [], [], 0\n",
    "    population = list(range(len(sampling_weights)))\n",
    "    for contexts in all_contexts:\n",
    "        negatives = []\n",
    "        while len(negatives) < len(contexts) * K:\n",
    "            if i == len(neg_candidates):\n",
    "                # 根据每个词的权重（sampling_weights）随机生成k个词的索引作为噪声词。\n",
    "                # 为了高效计算，可以将k设得稍大一点\n",
    "                i, neg_candidates = 0, random.choices(\n",
    "                    population, sampling_weights, k=int(1e5))\n",
    "            neg, i = neg_candidates[i], i + 1\n",
    "            # 噪声词不能是背景词\n",
    "            if neg not in set(contexts):\n",
    "                negatives.append(neg)\n",
    "        all_negatives.append(negatives)\n",
    "    return all_negatives\n",
    "\n",
    "sampling_weights = [counter[w]**0.75 for w in idx_to_token]\n",
    "all_negatives = get_negatives(all_contexts, sampling_weights, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#批量读取数据\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, centers, contexts, negatives):\n",
    "        assert len(centers) == len(contexts) == len(negatives)\n",
    "        self.centers = centers\n",
    "        self.contexts = contexts\n",
    "        self.negatives = negatives\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.centers[index], self.contexts[index], self.negatives[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.centers)\n",
    "    \n",
    "def batchify(data):\n",
    "    '''\n",
    "    用作DataLoader的参数collate_fn\n",
    "    @params:\n",
    "        data: 长为batch_size的列表，列表中的每个元素都是__getitem__得到的结果\n",
    "    @outputs:\n",
    "        batch: 批量化后得到 (centers, contexts_negatives, masks, labels) 元组\n",
    "            centers: 中心词下标，形状为 (n, 1) 的整数张量\n",
    "            contexts_negatives: 背景词和噪声词的下标，形状为 (n, m) 的整数张量\n",
    "            masks: 与补齐相对应的掩码，形状为 (n, m) 的0/1整数张量\n",
    "            labels: 指示中心词的标签，形状为 (n, m) 的0/1整数张量\n",
    "    '''\n",
    "    max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "    centers, contexts_negatives, masks, labels = [], [], [], []\n",
    "    for center, context, negative in data:\n",
    "        cur_len = len(context) + len(negative)\n",
    "        centers += [center]\n",
    "        contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "        masks += [[1] * cur_len + [0] * (max_len - cur_len)] # 使用掩码变量mask来避免填充项对损失函数计算的影响\n",
    "        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
    "        batch = (torch.tensor(centers).view(-1, 1), torch.tensor(contexts_negatives),\n",
    "            torch.tensor(masks), torch.tensor(labels))\n",
    "    return batch\n",
    "\n",
    "batch_size = 512\n",
    "num_workers = 0 if sys.platform.startswith('win32') else 4\n",
    "\n",
    "dataset = MyDataset(all_centers, all_contexts, all_negatives)\n",
    "data_iter = Data.DataLoader(dataset, batch_size, shuffle=True,\n",
    "                            collate_fn=batchify, \n",
    "                            num_workers=num_workers)\n",
    "for batch in data_iter:\n",
    "    for name, data in zip(['centers', 'contexts_negatives', 'masks',\n",
    "                           'labels'], batch):\n",
    "        print(name, 'shape:', data.shape)\n",
    "    break\n",
    "    \n",
    "#定义交叉熵损失函数\n",
    "    class SigmoidBinaryCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SigmoidBinaryCrossEntropyLoss, self).__init__()\n",
    "    def forward(self, inputs, targets, mask=None):\n",
    "        '''\n",
    "        @params:\n",
    "            inputs: 经过sigmoid层后为预测D=1的概率\n",
    "            targets: 0/1向量，1代表背景词，0代表噪音词\n",
    "        @return:\n",
    "            res: 平均到每个label的loss\n",
    "        '''\n",
    "        inputs, targets, mask = inputs.float(), targets.float(), mask.float()\n",
    "        res = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\", weight=mask)\n",
    "        res = res.sum(dim=1) / mask.float().sum(dim=1)\n",
    "        return res\n",
    "\n",
    "loss = SigmoidBinaryCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化参数并进行训练\n",
    "embed_size = 100\n",
    "net = nn.Sequential(nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size),\n",
    "                    nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size))\n",
    "\n",
    "#训练模型\n",
    "def train(net, lr, num_epochs):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"train on\", device)\n",
    "    net = net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        start, l_sum, n = time.time(), 0.0, 0\n",
    "        for batch in data_iter:\n",
    "            center, context_negative, mask, label = [d.to(device) for d in batch]\n",
    "            \n",
    "            pred = skip_gram(center, context_negative, net[0], net[1])\n",
    "            \n",
    "            l = loss(pred.view(label.shape), label, mask).mean() # 一个batch的平均loss\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            l_sum += l.cpu().item()\n",
    "            n += 1\n",
    "        print('epoch %d, loss %.2f, time %.2fs'\n",
    "              % (epoch + 1, l_sum / n, time.time() - start))\n",
    "\n",
    "def get_similar_tokens(query_token, k, embed):\n",
    "    '''\n",
    "    @params:\n",
    "        query_token: 给定的词语\n",
    "        k: 近义词的个数\n",
    "        embed: 预训练词向量\n",
    "    '''\n",
    "    W = embed.weight.data\n",
    "    x = W[token_to_idx[query_token]]\n",
    "    # 添加的1e-9是为了数值稳定性\n",
    "    cos = torch.matmul(W, x) / (torch.sum(W * W, dim=1) * torch.sum(x * x) + 1e-9).sqrt()\n",
    "    _, topk = torch.topk(cos, k=k+1)\n",
    "    topk = topk.cpu().numpy()\n",
    "    for i in topk[1:]:  # 除去输入词\n",
    "        print('cosine sim=%.3f: %s' % (cos[i], (idx_to_token[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.词嵌入进阶 \n",
    "\n",
    "W2V可以被进一步的改进\n",
    "\n",
    "### 子词嵌入\n",
    "\n",
    "FastText以固定大小的n-gram形式将单词更细致地表示为子词的集合；\n",
    "\n",
    "BPE(byte pair encoding)根据语料库信息，自动且动态地生成高频子词地集合\n",
    "\n",
    "### GloVe全局向量的词嵌入\n",
    "\n",
    "通过等价转换W2V模型的条件概率公式，得到一个全局的损失函数表达，并在此基础上进一步优化模型\n",
    "\n",
    "### GloVe实现示例\n",
    "\n",
    "词嵌入方法是想让模型学习出w$_j$有多大概率是w$_i$的背景词，真实标签是语料库的统计数据。\n",
    "\n",
    "word2vec损失函数:\n",
    "\n",
    "$\\sum_{t=1}^{T}\\sum_{-m<=j<=m}logP(w^(t+j)|w^t)$\n",
    "\n",
    "其中：$P(w_j|w_i)$=\\frac{exp(u^Tv_i)}{\\sum{k}exp(u_k^Tv_i}\n",
    "\n",
    "是$w_j$为中心词，$w_j$为背景词的skip-gram条件概率计算公式，可以简写为$q_{ij}$。\n",
    "\n",
    "上述损输函数的两个求和符号分别枚举了语料库中每个中心词和其对应的每个背景词，使用另一种计数方式（直接枚举每个词分别作为中心词和背景词的情况，可以表示为：\n",
    "$-\\sum_{i}\\sum{j}x_{ij}logq_ij$\n",
    "\n",
    "$-\\sum_{i}x_i\\sum_{j}p_{ij}logq_ij$\n",
    "\n",
    "上式中$x_ij$表示整个数据集中$w_j$作为$w_i$的背景词的次数总和；\n",
    "\n",
    "$x_i$是$w_i$的背景词窗大小总和；\n",
    "\n",
    "$p_{ij}=x_{ij}/x_i$是$w_j$在$w_i$的背景词窗中所占的比例。\n",
    "\n",
    "\n",
    "### GloVe模型\n",
    "\n",
    "在上述基础上做了几点改动：\n",
    "\n",
    "(1)使用非概率分布的变量$p_{ij}^1=x_{ij}$和$q_{ij}^1=exp(u_j^Tv_i)$,并对二者取对数\n",
    "\n",
    "(2)为每个词$w_i$增加两个标量模型参数，中心词偏差项$b_i$和背景词偏差项$c_i$，松弛了概率定义中的规范性\n",
    "\n",
    "(3)将每个损失项的权重$x_i$替代为$h(x_{ij})$,权重函数h(x)是值域在[0,1]熵的单调递增函数，送出了中心词重要性与$x_i$相关的隐含假设；\n",
    "\n",
    "(4)使用平方损失函数\n",
    "\n",
    "则损失函数可以表示为：\n",
    "\n",
    "$\\sum_{i}\\sum_{j}h(x_{ij})(u_j^Tv_i+b_i+c_i-logx_{ij})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#载入预先训练的GloVe向量，GloVe提供了多种规格的预训练词向量\n",
    "import torch\n",
    "import torchtext.vocab as vocab\n",
    "\n",
    "print([key for key in vocab.pretrained_aliases.keys() if \"glove\" in key])\n",
    "cache_dir = \"/home/kesci/input/GloVe6B5429\"\n",
    "glove = vocab.GloVe(name='6B', dim=50, cache=cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 求近义词和类比词\n",
    "\n",
    "近义词：此向量空间中的余弦相似性可以衡量词语含义的相似性，可以通过寻找空间的k近邻，查询单词的近义词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(W, x, k):\n",
    "    '''\n",
    "    @params:\n",
    "        W: 所有向量的集合\n",
    "        x: 给定向量\n",
    "        k: 查询的数量\n",
    "    @outputs:\n",
    "        topk: 余弦相似性最大k个的下标\n",
    "        [...]: 余弦相似度\n",
    "    '''\n",
    "    cos = torch.matmul(W, x.view((-1,))) / (\n",
    "        (torch.sum(W * W, dim=1) + 1e-9).sqrt() * torch.sum(x * x).sqrt())\n",
    "    _, topk = torch.topk(cos, k=k)\n",
    "    topk = topk.cpu().numpy()\n",
    "    return topk, [cos[i].item() for i in topk]\n",
    "\n",
    "def get_similar_tokens(query_token, k, embed):\n",
    "    '''\n",
    "    @params:\n",
    "        query_token: 给定的单词\n",
    "        k: 所需近义词的个数\n",
    "        embed: 预训练词向量\n",
    "    '''\n",
    "    topk, cos = knn(embed.vectors,\n",
    "                    embed.vectors[embed.stoi[query_token]], k+1)\n",
    "    for i, c in zip(topk[1:], cos[1:]):  # 除去输入词\n",
    "        print('cosine sim=%.3f: %s' % (c, (embed.itos[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 求类比词 \n",
    "\n",
    "如man类比woman。求解思路是，给定a,b,c，求解d满足：\n",
    "\n",
    "a之于b相当于c之于d，搜索vec(c)+vec(b)-vec(a)的结果向量最相似的词向量\n",
    "\n",
    "vec()表示词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analogy(token_a, token_b, token_c, embed):\n",
    "    '''\n",
    "    @params:\n",
    "        token_a: 词a\n",
    "        token_b: 词b\n",
    "        token_c: 词c\n",
    "        embed: 预训练词向量\n",
    "    @outputs:\n",
    "        res: 类比词d\n",
    "    '''\n",
    "    #stoi实现字符串转整数\n",
    "    vecs = [embed.vectors[embed.stoi[t]] \n",
    "                for t in [token_a, token_b, token_c]]\n",
    "    x = vecs[1] - vecs[0] + vecs[2]\n",
    "    topk, cos = knn(embed.vectors, x, 1)\n",
    "    res = embed.itos[topk[0]]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.文本情感分类\n",
    "\n",
    "将一段不定长的文本序列变换为文本的类别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchtext.vocab as Vocab\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#加载数据集，使用斯坦福的IMDb数据集（Stanford’s Large Movie Review Dataset）\n",
    "def read_imdb(folder='train', data_root=\"/home/kesci/input/IMDB2578/aclImdb_v1/aclImdb\"):\n",
    "    data = []\n",
    "    for label in ['pos', 'neg']:\n",
    "        folder_name = os.path.join(data_root, folder, label)\n",
    "        for file in tqdm(os.listdir(folder_name)):\n",
    "            with open(os.path.join(folder_name, file), 'rb') as f:\n",
    "                review = f.read().decode('utf-8').replace('\\n', '').lower()\n",
    "                data.append([review, 1 if label == 'pos' else 0])\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "DATA_ROOT = \"/home/kesci/input/IMDB2578/aclImdb_v1/\"\n",
    "data_root = os.path.join(DATA_ROOT, \"aclImdb\")\n",
    "train_data, test_data = read_imdb('train', data_root), read_imdb('test', data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#预处理数据，切分单词并创建词典\n",
    "def get_tokenized_imdb(data):\n",
    "    '''\n",
    "    @params:\n",
    "        data: 数据的列表，列表中的每个元素为 [文本字符串，0/1标签] 二元组\n",
    "    @return: 切分词后的文本的列表，列表中的每个元素为切分后的词序列\n",
    "    '''\n",
    "    def tokenizer(text):\n",
    "        return [tok.lower() for tok in text.split(' ')]\n",
    "    \n",
    "    return [tokenizer(review) for review, _ in data]\n",
    "\n",
    "def get_vocab_imdb(data):\n",
    "    '''\n",
    "    @params:\n",
    "        data: 同上\n",
    "    @return: 数据集上的词典，Vocab 的实例（freqs, stoi, itos）\n",
    "    '''\n",
    "    tokenized_data = get_tokenized_imdb(data)\n",
    "    counter = collections.Counter([tk for st in tokenized_data for tk in st])\n",
    "    return Vocab.Vocab(counter, min_freq=5)\n",
    "\n",
    "vocab = get_vocab_imdb(train_data)\n",
    "\n",
    "#将数据集文本从字符串形式转换为单词下标序列\n",
    "def preprocess_imdb(data, vocab):\n",
    "    '''\n",
    "    @params:\n",
    "        data: 同上，原始的读入数据\n",
    "        vocab: 训练集上生成的词典\n",
    "    @return:\n",
    "        features: 单词下标序列，形状为 (n, max_l) 的整数张量\n",
    "        labels: 情感标签，形状为 (n,) 的0/1整数张量\n",
    "    '''\n",
    "    max_l = 500  # 将每条评论通过截断或者补0，使得长度变成500\n",
    "\n",
    "    def pad(x):\n",
    "        return x[:max_l] if len(x) > max_l else x + [0] * (max_l - len(x))\n",
    "\n",
    "    tokenized_data = get_tokenized_imdb(data)\n",
    "    features = torch.tensor([pad([vocab.stoi[word] for word in words]) for words in tokenized_data])\n",
    "    labels = torch.tensor([score for _, score in data])\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建数据迭代器\n",
    "train_set = Data.TensorDataset(*preprocess_imdb(train_data, vocab))\n",
    "test_set = Data.TensorDataset(*preprocess_imdb(test_data, vocab))\n",
    "\n",
    "# 上面的代码等价于下面的注释代码\n",
    "# train_features, train_labels = preprocess_imdb(train_data, vocab)\n",
    "# test_features, test_labels = preprocess_imdb(test_data, vocab)\n",
    "# train_set = Data.TensorDataset(train_features, train_labels)\n",
    "# test_set = Data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "train_iter = Data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "test_iter = Data.DataLoader(test_set, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用双向RNN进行文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab, embed_size, num_hiddens, num_layers):\n",
    "        '''\n",
    "        @params:\n",
    "            vocab: 在数据集上创建的词典，用于获取词典大小\n",
    "            embed_size: 嵌入维度大小\n",
    "            num_hiddens: 隐藏状态维度大小\n",
    "            num_layers: 隐藏层个数\n",
    "        '''\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        \n",
    "        # encoder-decoder framework\n",
    "        # bidirectional设为True即得到双向循环神经网络\n",
    "        self.encoder = nn.LSTM(input_size=embed_size, \n",
    "                                hidden_size=num_hiddens, \n",
    "                                num_layers=num_layers,\n",
    "                                bidirectional=True)\n",
    "        self.decoder = nn.Linear(4*num_hiddens, 2) # 初始时间步和最终时间步的隐藏状态作为全连接层输入\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        @params:\n",
    "            inputs: 词语下标序列，形状为 (batch_size, seq_len) 的整数张量\n",
    "        @return:\n",
    "            outs: 对文本情感的预测，形状为 (batch_size, 2) 的张量\n",
    "        '''\n",
    "        # 因为LSTM需要将序列长度(seq_len)作为第一维，所以需要将输入转置\n",
    "        embeddings = self.embedding(inputs.permute(1, 0)) # (seq_len, batch_size, d)\n",
    "        # rnn.LSTM 返回输出、隐藏状态和记忆单元，格式如 outputs, (h, c)\n",
    "        outputs, _ = self.encoder(embeddings) # (seq_len, batch_size, 2*h)\n",
    "        encoding = torch.cat((outputs[0], outputs[-1]), -1) # (batch_size, 4*h)\n",
    "        outs = self.decoder(encoding) # (batch_size, 2)\n",
    "        return outs\n",
    "\n",
    "embed_size, num_hiddens, num_layers = 100, 100, 2\n",
    "net = BiRNN(vocab, embed_size, num_hiddens, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载预训练的词向量，由于预训练的词典及词语索引与使用的数据集不同\n",
    "#根据目前的词典和索引顺序加载预训练词向量\n",
    "cache_dir = \"/home/kesci/input/GloVe6B5429\"\n",
    "glove_vocab = Vocab.GloVe(name='6B', dim=100, cache=cache_dir)\n",
    "\n",
    "def load_pretrained_embedding(words, pretrained_vocab):\n",
    "    '''\n",
    "    @params:\n",
    "        words: 需要加载词向量的词语列表，以 itos (index to string) 的词典形式给出\n",
    "        pretrained_vocab: 预训练词向量\n",
    "    @return:\n",
    "        embed: 加载到的词向量\n",
    "    '''\n",
    "    embed = torch.zeros(len(words), pretrained_vocab.vectors[0].shape[0]) # 初始化为0\n",
    "    oov_count = 0 # out of vocabulary\n",
    "    for i, word in enumerate(words):\n",
    "        try:\n",
    "            idx = pretrained_vocab.stoi[word]\n",
    "            embed[i, :] = pretrained_vocab.vectors[idx]\n",
    "        except KeyError:\n",
    "            oov_count += 1\n",
    "    if oov_count > 0:\n",
    "        print(\"There are %d oov words.\" % oov_count)\n",
    "    return embed\n",
    "\n",
    "net.embedding.weight.data.copy_(load_pretrained_embedding(vocab.itos, glove_vocab))\n",
    "net.embedding.weight.requires_grad = False # 直接加载预训练好的, 所以不需要更新它"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练模型\n",
    "def evaluate_accuracy(data_iter, net, device=None):\n",
    "    if device is None and isinstance(net, torch.nn.Module):\n",
    "        device = list(net.parameters())[0].device \n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(net, torch.nn.Module):\n",
    "                net.eval()\n",
    "                acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n",
    "                net.train()\n",
    "            else:\n",
    "                if('is_training' in net.__code__.co_varnames):\n",
    "                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \n",
    "                else:\n",
    "                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n",
    "            n += y.shape[0]\n",
    "    return acc_sum / n\n",
    "\n",
    "def train(train_iter, test_iter, net, loss, optimizer, device, num_epochs):\n",
    "    net = net.to(device)\n",
    "    print(\"training on \", device)\n",
    "    batch_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y) \n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.cpu().item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))\n",
    "        \n",
    "        lr, num_epochs = 0.01, 5\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "train(train_iter, test_iter, net, loss, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#评估模型\n",
    "def predict_sentiment(net, vocab, sentence):\n",
    "    '''\n",
    "    @params：\n",
    "        net: 训练好的模型\n",
    "        vocab: 在该数据集上创建的词典，用于将给定的单词序转换为单词下标的序列，从而输入模型\n",
    "        sentence: 需要分析情感的文本，以单词序列的形式给出\n",
    "    @return: 预测的结果，positive 为正面情绪文本，negative 为负面情绪文本\n",
    "    '''\n",
    "    device = list(net.parameters())[0].device # 读取模型所在的环境\n",
    "    sentence = torch.tensor([vocab.stoi[word] for word in sentence], device=device)\n",
    "    label = torch.argmax(net(sentence.view((1, -1))), dim=1)\n",
    "    return 'positive' if label.item() == 1 else 'negative'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用卷积神经网络做文本分类 \n",
    "\n",
    "### TextCNN\n",
    "\n",
    "由一维卷积层和时序最大池化层组成\n",
    "\n",
    "TextCNN中的时序最大池化层实际对应一维全局最大池化层：假设输入包含多个通道，各通道由不同时间步上的数值组成，各通道的输出即该通道所有时间步中最大的数值。\n",
    "\n",
    "时序最大池化层的输入在各个通道上的时间步数可以不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一维卷积层\n",
    "def corr1d(X, K):\n",
    "    '''\n",
    "    @params:\n",
    "        X: 输入，形状为 (seq_len,) 的张量\n",
    "        K: 卷积核，形状为 (w,) 的张量\n",
    "    @return:\n",
    "        Y: 输出，形状为 (seq_len - w + 1,) 的张量\n",
    "    '''\n",
    "    w = K.shape[0] # 卷积窗口宽度\n",
    "    Y = torch.zeros((X.shape[0] - w + 1))\n",
    "    for i in range(Y.shape[0]): # 滑动窗口\n",
    "        Y[i] = (X[i: i + w] * K).sum()\n",
    "    return Y\n",
    "#多通道一维互相关运算\n",
    "def corr1d_multi_in(X, K):\n",
    "    # 首先沿着X和K的通道维遍历并计算一维互相关结果。然后将所有结果堆叠起来沿第0维累加\n",
    "    return torch.stack([corr1d(x, k) for x, k in zip(X, K)]).sum(dim=0)\n",
    "    # [corr1d(X[i], K[i]) for i in range(X.shape[0])]\n",
    "\n",
    "#时序池化层\n",
    "class GlobalMaxPool1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalMaxPool1d, self).__init__()\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        @params:\n",
    "            x: 输入，形状为 (batch_size, n_channels, seq_len) 的张量\n",
    "        @return: 时序最大池化后的结果，形状为 (batch_size, n_channels, 1) 的张量\n",
    "        '''\n",
    "        return F.max_pool1d(x, kernel_size=x.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设输入的文本序列由  n  个词组成，每个词用  d  维的词向量表示。那么输入样本的宽为  n ，输入通道数为  d 。TextCNN 的计算主要分为以下几步。\n",
    "\n",
    "(1)定义多个一维卷积核，并使用这些卷积核对输入分别做卷积计算。宽度不同的卷积核可能会捕捉到不同个数的相邻词的相关性。\n",
    "\n",
    "(2)对输出的所有通道分别做时序最大池化，再将这些通道的池化输出值连结为向量。\n",
    "\n",
    "(3)通过全连接层将连结后的向量变换为有关各类别的输出。这一步可以使用dropout应对过拟合。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义模型\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab, embed_size, kernel_sizes, num_channels):\n",
    "        '''\n",
    "        @params:\n",
    "            vocab: 在数据集上创建的词典，用于获取词典大小\n",
    "            embed_size: 嵌入维度大小\n",
    "            kernel_sizes: 卷积核大小列表\n",
    "            num_channels: 卷积通道数列表\n",
    "        '''\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size) # 参与训练的嵌入层\n",
    "        self.constant_embedding = nn.Embedding(len(vocab), embed_size) # 不参与训练的嵌入层\n",
    "        \n",
    "        self.pool = GlobalMaxPool1d() # 时序最大池化层没有权重，所以可以共用一个实例\n",
    "        self.convs = nn.ModuleList()  # 创建多个一维卷积层\n",
    "        for c, k in zip(num_channels, kernel_sizes):\n",
    "            self.convs.append(nn.Conv1d(in_channels = 2*embed_size, \n",
    "                                        out_channels = c, \n",
    "                                        kernel_size = k))\n",
    "            \n",
    "        self.decoder = nn.Linear(sum(num_channels), 2)\n",
    "        self.dropout = nn.Dropout(0.5) # 丢弃层用于防止过拟合\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        @params:\n",
    "            inputs: 词语下标序列，形状为 (batch_size, seq_len) 的整数张量\n",
    "        @return:\n",
    "            outputs: 对文本情感的预测，形状为 (batch_size, 2) 的张量\n",
    "        '''\n",
    "        embeddings = torch.cat((\n",
    "            self.embedding(inputs), \n",
    "            self.constant_embedding(inputs)), dim=2) # (batch_size, seq_len, 2*embed_size)\n",
    "        # 根据一维卷积层要求的输入格式，需要将张量进行转置\n",
    "        embeddings = embeddings.permute(0, 2, 1) # (batch_size, 2*embed_size, seq_len)\n",
    "        \n",
    "        encoding = torch.cat([\n",
    "            self.pool(F.relu(conv(embeddings))).squeeze(-1) for conv in self.convs], dim=1)\n",
    "        # encoding = []\n",
    "        # for conv in self.convs:\n",
    "        #     out = conv(embeddings) # (batch_size, out_channels, seq_len-kernel_size+1)\n",
    "        #     out = self.pool(F.relu(out)) # (batch_size, out_channels, 1)\n",
    "        #     encoding.append(out.squeeze(-1)) # (batch_size, out_channels)\n",
    "        # encoding = torch.cat(encoding) # (batch_size, out_channels_sum)\n",
    "        \n",
    "        # 应用丢弃法后使用全连接层得到输出\n",
    "        outputs = self.decoder(self.dropout(encoding))\n",
    "        return outputs\n",
    "\n",
    "embed_size, kernel_sizes, nums_channels = 100, [3, 4, 5], [100, 100, 100]\n",
    "net = TextCNN(vocab, embed_size, kernel_sizes, nums_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "train(train_iter, test_iter, net, loss, optimizer, device, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
