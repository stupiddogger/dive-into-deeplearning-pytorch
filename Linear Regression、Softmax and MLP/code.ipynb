{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 动手学深度学习(1)\n",
    "\n",
    "课程地址：https://www.boyuai.com/elites/course/cZu18YmweLv10OeV\n",
    "资源地址：https://www.boyuai.com/elites/course/cZu18YmweLv10OeV\n",
    "基于pytorch实现\n",
    "\n",
    "##  1.线性回归\n",
    "\n",
    "使用房屋数据集进行建模，特征包括房屋的面积和房龄，输出为房屋的价格，损失函数采用平方损失函数，优化方法使用随机梯度下降\n",
    "\n",
    "### 1.1从零实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000])\n",
      "torch.Size([1000, 2])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQjzO/weOjkY6uKCwvgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#随机生成数据集\n",
    "#设置输入特征数量与样本数量\n",
    "num_inputs=2\n",
    "num_examples=1000\n",
    "#利用真实的权重和偏执生成对应的标签\n",
    "true_w=[2,-3,4]\n",
    "true_b=4.2\n",
    "features=torch.randn(num_examples,num_inputs,dtype=torch.float32)\n",
    "labels=true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\n",
    "#添加噪声\n",
    "labels+=torch.tensor(np.random.normal(0,0.01,size=labels.size()),dtype=torch.float32)\n",
    "#对数据进行可视化\n",
    "plt.scatter(features[:,1].numpy(),labels.numpy(),1)\n",
    "print(labels.shape)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3503, -0.7891],\n",
      "        [ 0.4158, -0.1140],\n",
      "        [-0.8204,  1.0844],\n",
      "        [ 0.5934, -1.6101],\n",
      "        [ 1.1703, -0.0218],\n",
      "        [ 0.8503,  0.8522],\n",
      "        [ 0.6622,  0.1193],\n",
      "        [-0.7362,  0.2037],\n",
      "        [ 0.4910, -0.3080],\n",
      "        [-0.1510,  1.9247]]) \n",
      " tensor([ 3.8606,  5.3696, -0.6965, 10.2308,  6.6187,  3.3394,  5.1524,  2.1107,\n",
      "         6.1214, -1.8702])\n"
     ]
    }
   ],
   "source": [
    "#读取数据集\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    random.shuffle(indices)  # random read 10 samples\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        j = torch.LongTensor(indices[i: min(i + batch_size, num_examples)]) # the last time may be not enough for a whole batch\n",
    "        yield  features.index_select(0, j), labels.index_select(0, j)\n",
    "batch_size = 10\n",
    "\n",
    "for X, y in data_iter(batch_size, features, labels):\n",
    "    print(X, '\\n', y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.026786\n",
      "epoch 2, loss 0.000095\n",
      "epoch 3, loss 0.000048\n",
      "epoch 4, loss 0.000048\n",
      "epoch 5, loss 0.000048\n"
     ]
    }
   ],
   "source": [
    "#初始化模型参数\n",
    "w=torch.tensor(np.random.normal(0,0.01,(num_inputs,1)),dtype=torch.float32)\n",
    "b=torch.zeros(1,dtype=torch.float32)\n",
    "#给参数w和b赋梯度\n",
    "w.requires_grad_(requires_grad=True)\n",
    "b.requires_grad_(requires_grad=True)\n",
    "\n",
    "#定义模型y=W*X+b\n",
    "def linreg(X,w,b):\n",
    "    return torch.mm(X,w)+b\n",
    "\n",
    "#定义损失函数\n",
    "def squared_loss(y_hat,y):\n",
    "    return (y_hat-y.view(y_hat.size()))**2/2\n",
    "\n",
    "#定义SGD优化函数\n",
    "def sgd(params,lr,batch_size):\n",
    "    for param in params:\n",
    "        param.data-=lr*param.grad/batch_size\n",
    "        \n",
    "#训练，初始化学习率lr，迭代次数num_epochs\n",
    "lr=0.03\n",
    "num_epochs = 5\n",
    "net=linreg\n",
    "loss=squared_loss\n",
    "for epoch in range(num_epochs):\n",
    "    for X,y in data_iter(batch_size,features,labels):\n",
    "        l=loss(net(X,w,b),y).sum()\n",
    "        l.backward()\n",
    "        sgd([w,b],lr,batch_size)\n",
    "        #参数梯度清零，避免累加\n",
    "        w.grad.data.zero_()\n",
    "        b.grad.data.zero_()\n",
    "    train_l=loss(net(features,w,b),labels)\n",
    "    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2.0002],\n",
       "         [-3.0002]], requires_grad=True),\n",
       " [2, -3, 4],\n",
       " tensor([4.1995], requires_grad=True),\n",
       " 4.2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, true_w, b, true_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2简单实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5461,  0.1822],\n",
      "        [-0.3892,  0.3422],\n",
      "        [-1.1618, -0.2895],\n",
      "        [ 0.6008, -1.1407],\n",
      "        [-0.1354, -0.9512],\n",
      "        [ 0.8827,  1.4615],\n",
      "        [-0.6715,  0.9073],\n",
      "        [ 0.6681, -0.2326],\n",
      "        [-0.1517,  0.4691],\n",
      "        [ 0.2303,  1.7655]]) \n",
      " tensor([ 2.4840,  2.2525,  2.8591,  9.2632,  7.1861,  0.9929, -0.2172,  6.3230,\n",
      "         2.3050, -1.3411])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch.utils.data as Data\n",
    "#生成数据\n",
    "num_inputs = 2\n",
    "num_examples = 1000\n",
    "\n",
    "true_w = [2, -3.4]\n",
    "true_b = 4.2\n",
    "\n",
    "features = torch.tensor(np.random.normal(0, 1, (num_examples, num_inputs)), dtype=torch.float)\n",
    "labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\n",
    "labels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)\n",
    "batch_size = 10\n",
    "# 将特征和标签组合成dataset\n",
    "dataset = Data.TensorDataset(features, labels)\n",
    "# 将dataset输入DataLoader\n",
    "data_iter = Data.DataLoader(\n",
    "    dataset=dataset,            \n",
    "    batch_size=batch_size,      \n",
    "    shuffle=True,               # 是否捣乱数据\n",
    "    num_workers=2,              # 多线程读取数据\n",
    ")\n",
    "for X, y in data_iter:\n",
    "    print(X, '\\n', y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearNet(\n",
      "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#定义模型\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self,n_feature):\n",
    "        super(LinearNet,self).__init__()\n",
    "        self.linear=nn.Linear(n_feature,1)    #函数原型\n",
    "    def forward(self,x):\n",
    "        y=self.linear(x)\n",
    "        return y\n",
    "net=LinearNet(num_inputs)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0007, -0.0073]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#初始化模型\n",
    "net = nn.Sequential()\n",
    "net.add_module('linear', nn.Linear(num_inputs, 1))\n",
    "#初始化参数\n",
    "from torch.nn import init\n",
    "init.normal_(net[0].weight,mean=0.0,std=0.01)\n",
    "init.constant_(net[0].bias,val=0.0)\n",
    "for param in net.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.03\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#定义均方差损失函数\n",
    "loss=nn.MSELoss()\n",
    "#定义优化函数\n",
    "import torch.optim as optim\n",
    "optimizer=optim.SGD(net.parameters(),lr=0.03)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 0.000410\n",
      "epoch 2, loss: 0.000112\n",
      "epoch 3, loss: 0.000090\n",
      "[2, -3.4] tensor([[ 1.9995, -3.4009]])\n",
      "4.2 tensor([4.1997])\n"
     ]
    }
   ],
   "source": [
    "#训练\n",
    "num_epochs=3\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    for X,y in data_iter:\n",
    "        output=net(X)\n",
    "        l=loss(output,y.view(-1,1))\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    print('epoch %d, loss: %f' % (epoch, l.item()))\n",
    "#结果比较\n",
    "dense = net[0]\n",
    "print(true_w, dense.weight.data)\n",
    "print(true_b, dense.bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.softmax和分类模型\n",
    "\n",
    "使用MNIST数据集进行多分类，交叉熵损失函数\n",
    "\n",
    "### 2.1 加载数据\n",
    "\n",
    "pytorch中主要包括：\n",
    "(1) torchvision.datasets：包含一些加载数据的函数及常用的数据集接口\n",
    "(2) torchvision.models：包含常用的模型结构(预训练模型)\n",
    "(3) torchvision.utils：包含常用的图片变换方法，如裁剪、旋转等\n",
    "(4) torchvision.utils：其他方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed package\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import sys\n",
    "def load_mnist(path, kind='train'):\n",
    "    import os\n",
    "    import gzip\n",
    "    import numpy as np\n",
    "\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte.gz'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte.gz'\n",
    "                               % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels\n",
    "X_train, y_train = load_mnist('C:/Users/wangk/Desktop/DIVE_INTO_DEEP_LEARNING', kind='train')\n",
    "X_test, y_test = load_mnist('C:/Users/wangk/Desktop/DIVE_INTO_DEEP_LEARNING', kind='t10k')\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "#转换数据\n",
    "X_train = torch.tensor(X_train.reshape(60000,28,28), dtype=torch.float)\n",
    "X_test = torch.tensor(X_test.reshape(10000,28,28), dtype=torch.float)\n",
    "y_train = torch.tensor(y_train, dtype=torch.int64)\n",
    "y_test = torch.tensor(y_test, dtype=torch.int64)\n",
    "dataset_train = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_iter = torch.utils.data.DataLoader(dataset_train, batch_size, shuffle=True)\n",
    "dataset_test = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "test_iter = torch.utils.data.DataLoader(dataset_test, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 softmax从零实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import sys\n",
    "#初始化参数\n",
    "#28*28像素图像\n",
    "num_inputs=784\n",
    "#10个类别\n",
    "num_outputs = 10\n",
    "#权重和偏置\n",
    "W = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_outputs)), dtype=torch.float)\n",
    "b = torch.zeros(num_outputs, dtype=torch.float)\n",
    "W.requires_grad_(requires_grad=True)\n",
    "b.requires_grad_(requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss nan, train acc 0.100, test acc 0.100\n",
      "epoch 2, loss nan, train acc 0.100, test acc 0.100\n",
      "epoch 3, loss nan, train acc 0.100, test acc 0.100\n",
      "epoch 4, loss nan, train acc 0.100, test acc 0.100\n",
      "epoch 5, loss nan, train acc 0.100, test acc 0.100\n"
     ]
    }
   ],
   "source": [
    "#定义softmax\n",
    "def softmax(X):\n",
    "    X_exp=X.exp()\n",
    "    patition=X_exp.sum(dim=1,keepdim=True)\n",
    "    return X_exp/patition\n",
    "#定义模型\n",
    "#X.view（）函数将X进行拉伸，(-1,n)拉伸为列，（-n,1)拉伸为行\n",
    "def net(X):\n",
    "    return softmax(torch.mm(X.view((-1, num_inputs)), W) + b)\n",
    "#定义损失函数\n",
    "def cross_entropy(y_hat,y):\n",
    "    return - torch.log(y_hat.gather(1,y.view(-1,1)))\n",
    "#定义模型准确率\n",
    "def accuracy(y_hat,y):\n",
    "    return (y_hat.argmax(dim=1)==y).float().mean().item()\n",
    "def evaluate_accuracy(data_iter,net):\n",
    "    acc_sum,n=0.0,0\n",
    "    for X,y in data_iter:\n",
    "        acc_sum+=(net(X).argmax(dim=1) == y).float().sum().item()\n",
    "        n+=y.shape[0]\n",
    "    return acc_sum/n\n",
    "#定义优化函数\n",
    "def sgd(params,lr,batch_size):\n",
    "    for param in params:\n",
    "        param.data-=lr*param.grad/batch_size\n",
    "        \n",
    "def get_fashion_mnist_labels(labels):\n",
    "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    return [text_labels[int(i)] for i in labels]\n",
    "def show_fashion_mnist(images, labels):\n",
    "    #d2l.use_svg_display()\n",
    "    # 这里的_表示我们忽略（不使用）的变量\n",
    "    _, figs = plt.subplots(1, len(images), figsize=(12, 12))\n",
    "    for f, img, lbl in zip(figs, images, labels):\n",
    "        f.imshow(img.view((28, 28)).numpy())\n",
    "        f.set_title(lbl)\n",
    "        f.axes.get_xaxis().set_visible(False)\n",
    "        f.axes.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "\n",
    "        \n",
    "#模型训练\n",
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,\n",
    "              params=None, lr=None, optimizer=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        for X, y in train_iter:\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y).sum()\n",
    "            # 梯度清零\n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "            elif params is not None and params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "            l.backward()\n",
    "            if optimizer is None:\n",
    "                sgd(params, lr, batch_size)\n",
    "            else:\n",
    "                optimizer.step() \n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n",
    "train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size, [W, b], lr)\n",
    "\n",
    "#模型预测\n",
    "X, y = iter(test_iter).next()\n",
    "true_labels = get_fashion_mnist_labels(y.numpy())\n",
    "pred_labels = get_fashion_mnist_labels(net(X).argmax(dim=1).numpy())\n",
    "titles = [true + '\\n' + pred for true, pred in zip(true_labels, pred_labels)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3简洁实现 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\n",
    "#定义模型\n",
    "num_inputs=784\n",
    "num_outputs=10\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self,num_inputs,num_outputs):\n",
    "        super(Linear,self).__init__()\n",
    "        self.linear=nn.Linear(num_inputs,num_outputs)\n",
    "    def forward(self,x):\n",
    "        y=self.linear(x.view(x.shape[0],-1))\n",
    "        return y\n",
    "#将图片铺平\n",
    "class FlattenLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FlattenLayer,self).__init__()\n",
    "    def forward(self,x):\n",
    "        return x.view(x.shape[0],-1)\n",
    "from collections import OrderedDict\n",
    "net = nn.Sequential(\n",
    "        # FlattenLayer(),\n",
    "        # LinearNet(num_inputs, num_outputs) \n",
    "        OrderedDict([\n",
    "           ('flatten', FlattenLayer()),\n",
    "           ('linear', nn.Linear(num_inputs, num_outputs))]))\n",
    "#初始化模型参数\n",
    "init.normal_(net.linear.weight, mean=0, std=0.01)\n",
    "init.constant_(net.linear.bias, val=0)\n",
    "#定义损失函数\n",
    "loss=nn.CrossEntropyLoss()\n",
    "#定义优化器\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 95.3254, train acc 0.679, test acc 0.684\n",
      "epoch 2, loss 49.0642, train acc 0.753, test acc 0.739\n",
      "epoch 3, loss 46.2734, train acc 0.763, test acc 0.640\n",
      "epoch 4, loss 46.3941, train acc 0.765, test acc 0.748\n",
      "epoch 5, loss 41.8360, train acc 0.778, test acc 0.721\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.MLP\n",
    "\n",
    "仍使用FashionMinist\n",
    "\n",
    "### 3.1从零实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "#加载数据\n",
    "def load_mnist(path, kind='train'):\n",
    "    import os\n",
    "    import gzip\n",
    "    import numpy as np\n",
    "\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte.gz'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte.gz'\n",
    "                               % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels\n",
    "X_train, y_train = load_mnist('C:/Users/wangk/Desktop/DIVE_INTO_DEEP_LEARNING', kind='train')\n",
    "X_test, y_test = load_mnist('C:/Users/wangk/Desktop/DIVE_INTO_DEEP_LEARNING', kind='t10k')\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "#转换数据\n",
    "X_train = torch.tensor(X_train.reshape(60000,28,28), dtype=torch.float)\n",
    "X_test = torch.tensor(X_test.reshape(10000,28,28), dtype=torch.float)\n",
    "y_train = torch.tensor(y_train, dtype=torch.int64)\n",
    "y_test = torch.tensor(y_test, dtype=torch.int64)\n",
    "dataset_train = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_iter = torch.utils.data.DataLoader(dataset_train, batch_size, shuffle=True)\n",
    "dataset_test = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "test_iter = torch.utils.data.DataLoader(dataset_test, batch_size, shuffle=True)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 10563564.3088, train acc 0.099, test acc 0.100\n",
      "epoch 2, loss 0.0090, train acc 0.100, test acc 0.100\n",
      "epoch 3, loss 0.0090, train acc 0.097, test acc 0.100\n",
      "epoch 4, loss 0.0090, train acc 0.099, test acc 0.100\n",
      "epoch 5, loss 0.0090, train acc 0.100, test acc 0.100\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "#定义模型参数\n",
    "num_inputs,numputs,num_hiddens=784,10,256\n",
    "W1 = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_hiddens)), dtype=torch.float)\n",
    "b1 = torch.zeros(num_hiddens, dtype=torch.float)\n",
    "W2 = torch.tensor(np.random.normal(0, 0.01, (num_hiddens, num_outputs)), dtype=torch.float)\n",
    "b2 = torch.zeros(num_outputs, dtype=torch.float)\n",
    "params = [W1, b1, W2, b2]\n",
    "for param in params:\n",
    "    param.requires_grad_(requires_grad=True)\n",
    "#定义激活函数\n",
    "def relu(X):\n",
    "    return torch.max(input=X,other=torch.tensor(0.0))\n",
    "#定义模型\n",
    "def net(X):\n",
    "    X = X.view((-1, num_inputs))\n",
    "    H = relu(torch.matmul(X, W1) + b1)\n",
    "    return torch.matmul(H, W2) + b2\n",
    "#定义损失函数\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "#定义优化函数\n",
    "def sgd(params,lr,batch_size):\n",
    "    for param in params:\n",
    "        param.data-=lr*param.grad/batch_size\n",
    "#定义模型准确率\n",
    "def accuracy(y_hat,y):\n",
    "    return (y_hat.argmax(dim=1)==y).float().mean().item()\n",
    "def evaluate_accuracy(data_iter,net):\n",
    "    acc_sum,n=0.0,0\n",
    "    for X,y in data_iter:\n",
    "        acc_sum+=(net(X).argmax(dim=1) == y).float().sum().item()\n",
    "        n+=y.shape[0]\n",
    "    return acc_sum/n\n",
    "#训练\n",
    "num_epochs, lr = 5, 100.0\n",
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,\n",
    "              params=None, lr=None, optimizer=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        for X, y in train_iter:\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y).sum()\n",
    "            # 梯度清零\n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "            elif params is not None and params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "            l.backward()\n",
    "            if optimizer is None:\n",
    "                sgd(params, lr, batch_size)\n",
    "            else:\n",
    "                optimizer.step() \n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n",
    "train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2简洁实现\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载数据\n",
    "def load_mnist(path, kind='train'):\n",
    "    import os\n",
    "    import gzip\n",
    "    import numpy as np\n",
    "\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte.gz'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte.gz'\n",
    "                               % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels\n",
    "X_train, y_train = load_mnist('C:/Users/wangk/Desktop/DIVE_INTO_DEEP_LEARNING', kind='train')\n",
    "X_test, y_test = load_mnist('C:/Users/wangk/Desktop/DIVE_INTO_DEEP_LEARNING', kind='t10k')\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "#转换数据\n",
    "X_train = torch.tensor(X_train.reshape(60000,28,28), dtype=torch.float)\n",
    "X_test = torch.tensor(X_test.reshape(10000,28,28), dtype=torch.float)\n",
    "y_train = torch.tensor(y_train, dtype=torch.int64)\n",
    "y_test = torch.tensor(y_test, dtype=torch.int64)\n",
    "dataset_train = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_iter = torch.utils.data.DataLoader(dataset_train, batch_size, shuffle=True)\n",
    "dataset_test = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "test_iter = torch.utils.data.DataLoader(dataset_test, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 153078.0437, train acc 0.101, test acc 0.100\n",
      "epoch 2, loss 0.0090, train acc 0.099, test acc 0.100\n",
      "epoch 3, loss 0.0090, train acc 0.100, test acc 0.100\n",
      "epoch 4, loss 0.0090, train acc 0.098, test acc 0.100\n",
      "epoch 5, loss 0.0090, train acc 0.098, test acc 0.100\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,\n",
    "              params=None, lr=None, optimizer=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        for X, y in train_iter:\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y).sum()\n",
    "            # 梯度清零\n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "            elif params is not None and params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "            l.backward()\n",
    "            if optimizer is None:\n",
    "                sgd(params, lr, batch_size)\n",
    "            else:\n",
    "                optimizer.step() \n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n",
    "\n",
    "\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self,num_inputs,num_outputs):\n",
    "        super(Linear,self).__init__()\n",
    "        self.linear=nn.Linear(num_inputs,num_outputs)\n",
    "    def forward(self,x):\n",
    "        y=self.linear(x.view(x.shape[0],-1))\n",
    "        return y\n",
    "#将图片铺平\n",
    "class FlattenLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FlattenLayer,self).__init__()\n",
    "    def forward(self,x):\n",
    "        return x.view(x.shape[0],-1)\n",
    "\n",
    "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
    "    \n",
    "net = nn.Sequential(\n",
    "        FlattenLayer(),\n",
    "        nn.Linear(num_inputs, num_hiddens),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(num_hiddens, num_outputs), \n",
    "        )\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.5)\n",
    "#初始化参数    \n",
    "for params in net.parameters():\n",
    "    init.normal_(params, mean=0, std=0.01)\n",
    "#训练仍使用前述train_ch3函数\n",
    "num_epochs = 5\n",
    "train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
